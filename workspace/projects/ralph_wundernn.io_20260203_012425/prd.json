{
  "task": "**\nDevelop and export a PyTorch-based multi-step time-series forecasting model that ingests sequences of 1,000 timesteps, uses the first 99 as context to autoregressively predict the remaining 901 steps, and is packaged according to a specific competition interface (in `solution.py`) with an ONNX runtime export (`model.onnx`) to compete in the Wundernn.io Predictorium for a top 10% leaderboard position.",
  "domain": "time_series_forecasting",
  "total_items": 10,
  "user_stories": [
    {
      "id": "ML-001",
      "title": "Environment setup and dependencies",
      "description": "Create Python 3.11 venv, install PyTorch, ONNX Runtime, Pandas, and competition requirements.",
      "acceptance_criteria": [
        "Python 3.11 virtual environment created",
        "requirements.txt with pinned versions: torch>=2.0, onnxruntime, pandas, pyarrow, numpy, tqdm",
        "All packages install without errors",
        "Python version verified: 3.11.x",
        ".gitignore includes venv/, __pycache__/, *.pyc"
      ],
      "verification": "python --version && pip list | grep -E '(torch|onnx|pandas)' && python -c 'import torch; print(torch.__version__)'",
      "why": "Match competition Docker environment (python:3.11-slim-bookworm). Ensure reproducibility.",
      "files_touched": [
        "requirements.txt",
        ".gitignore",
        "README.md"
      ],
      "status": "failed",
      "attempts": 20,
      "errors": [
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: ",
        "Command failed with code 1: "
      ]
    },
    {
      "id": "ML-002",
      "title": "Download and verify starter pack",
      "description": "Download competition data, extract to data/raw/, verify file integrity and data shapes.",
      "acceptance_criteria": [
        "Starter pack downloaded from competition URL",
        "Files extracted: train.parquet, valid.parquet, utils.py, baseline.onnx",
        "train.parquet shape verified: 10,721 sequences",
        "valid.parquet shape verified: 1,444 sequences",
        "Each sequence has 1000 timesteps confirmed",
        "Script: scripts/download_data.py for reproducibility"
      ],
      "verification": "ls data/raw/datasets/ && python -c \"import pandas as pd; df = pd.read_parquet('data/raw/datasets/train.parquet'); print(f'Train: {df.shape}'); assert len(df) == 10721\"",
      "why": "Need training/validation data to start model development. Verify data integrity before training.",
      "files_touched": [
        "scripts/download_data.py",
        "data/raw/datasets/train.parquet",
        "data/raw/datasets/valid.parquet",
        "data/raw/utils.py"
      ],
      "status": "todo",
      "attempts": 0,
      "errors": []
    },
    {
      "id": "ML-003",
      "title": "PyTorch DataLoader implementation",
      "description": "Implement Dataset class for time-series sequences with 99-step context and 900-step prediction targets.",
      "acceptance_criteria": [
        "src/data_loader.py implements torch.utils.data.Dataset",
        "Correctly splits sequences: context=[0:99], target=[99:999]",
        "Returns (context, target) tuples with correct shapes",
        "Supports both train and validation datasets",
        "Optional data augmentation (noise, scaling) implemented",
        "Unit tests pass: tests/test_data_loader.py",
        "Docstrings with type hints on all methods"
      ],
      "verification": "python -c \"from src.data_loader import TimeSeriesDataset; ds = TimeSeriesDataset('data/raw/datasets/train.parquet'); print(f'Dataset size: {len(ds)}'); ctx, tgt = ds[0]; print(f'Context: {ctx.shape}, Target: {tgt.shape}'); assert ctx.shape[0] == 99\" && pytest tests/test_data_loader.py -v",
      "why": "Efficient batching and data loading for training. Correct context/target split is critical for model performance.",
      "files_touched": [
        "src/data_loader.py",
        "tests/test_data_loader.py"
      ],
      "status": "todo",
      "attempts": 0,
      "errors": []
    },
    {
      "id": "ML-004",
      "title": "Baseline model evaluation",
      "description": "Load baseline.onnx with ONNX Runtime, run inference on validation set, record Pearson Correlation score.",
      "acceptance_criteria": [
        "src/evaluate_baseline.py loads baseline.onnx successfully",
        "Runs inference on all 1,444 validation sequences",
        "Calculates Pearson Correlation using utils.py scoring function",
        "Logs baseline score to models/baseline_score.txt",
        "Measures and logs inference time (<10s requirement check)",
        "Script outputs: 'Baseline Pearson Correlation: 0.XXXX, Time: Xs'"
      ],
      "verification": "python src/evaluate_baseline.py && cat models/baseline_score.txt",
      "why": "Establish performance target to beat. Baseline provides reference for model improvements.",
      "files_touched": [
        "src/evaluate_baseline.py",
        "models/baseline_score.txt"
      ],
      "status": "todo",
      "attempts": 0,
      "errors": []
    },
    {
      "id": "ML-005",
      "title": "Enhanced model architecture",
      "description": "Design and implement improved LSTM/GRU model with attention or Transformer encoder to beat baseline.",
      "acceptance_criteria": [
        "src/model.py implements PredictionModel class (competition interface)",
        "Model architecture: Bidirectional LSTM + Multi-Head Attention OR GRU + Skip Connections",
        "Input shape: (batch, 99, num_features), Output: (batch, 900)",
        "Model parameters: 500K-2M (balance capacity vs speed)",
        "Supports gradient checkpointing for memory efficiency",
        "Unit tests verify forward pass: tests/test_model.py",
        "Docstrings explain architecture choices"
      ],
      "verification": "python -c \"from src.model import PredictionModel; m = PredictionModel(); params = sum(p.numel() for p in m.parameters()); print(f'Parameters: {params:,}'); assert 500_000 <= params <= 2_000_000\" && pytest tests/test_model.py -v",
      "why": "Need improved architecture to beat baseline GRU. Attention/skip connections capture long-range dependencies in time-series.",
      "files_touched": [
        "src/model.py",
        "tests/test_model.py"
      ],
      "status": "todo",
      "attempts": 0,
      "errors": []
    },
    {
      "id": "ML-006",
      "title": "Training pipeline with checkpointing",
      "description": "Implement full training loop with AdamW optimizer, learning rate scheduling, early stopping, and model checkpointing.",
      "acceptance_criteria": [
        "src/train.py implements training loop",
        "Loss: MSE or Huber loss",
        "Optimizer: AdamW with cosine annealing LR scheduler",
        "Batch size: 64 (configurable via CLI args)",
        "Epochs: 100 max with early stopping (patience=10 on validation loss)",
        "Saves best model checkpoint to models/checkpoints/best_model.pt",
        "Logs train/val loss every epoch to CSV or TensorBoard",
        "GPU/CPU auto-detection and utilization",
        "Reproducibility: sets torch/numpy/random seeds",
        "Progress bar with tqdm"
      ],
      "verification": "python src/train.py --epochs 5 --batch_size 32 --seed 42 && ls models/checkpoints/best_model.pt",
      "why": "Train model on 10,721 sequences efficiently. Checkpointing prevents loss of progress. Early stopping avoids overfitting.",
      "files_touched": [
        "src/train.py",
        "src/trainer.py",
        "config/train_config.yaml",
        "tests/test_training.py"
      ],
      "status": "todo",
      "attempts": 0,
      "errors": []
    },
    {
      "id": "ML-007",
      "title": "Model evaluation with metrics",
      "description": "Evaluate trained model on validation set, calculate Pearson Correlation (primary metric) plus RMSE, MAE, R\u00b2.",
      "acceptance_criteria": [
        "src/evaluate.py loads trained PyTorch model from checkpoint",
        "Runs inference on validation set (1,444 sequences)",
        "Calculates Pearson Correlation using competition's utils.py",
        "Also computes: RMSE, MAE, R\u00b2 for analysis",
        "Saves predictions to CSV: output/predictions.csv",
        "Generates plots: predicted vs actual, residuals distribution",
        "Outputs: 'Validation Pearson: 0.XXXX (Baseline: 0.YYYY, Beat: True/False)'",
        "Evaluation results saved to models/evaluation_report.json"
      ],
      "verification": "python src/evaluate.py --model models/checkpoints/best_model.pt && cat models/evaluation_report.json",
      "why": "Competition metric is Pearson Correlation. Need to verify model beats baseline before ONNX export.",
      "files_touched": [
        "src/evaluate.py",
        "notebooks/evaluation_analysis.ipynb",
        "models/evaluation_report.json"
      ],
      "status": "todo",
      "attempts": 0,
      "errors": []
    },
    {
      "id": "ML-008",
      "title": "ONNX model export and optimization",
      "description": "Convert trained PyTorch model to ONNX format, verify inference speed <10s, validate output accuracy.",
      "acceptance_criteria": [
        "src/export_onnx.py converts PyTorch checkpoint to ONNX",
        "Exported model: models/final/model.onnx",
        "ONNX model tested with onnxruntime inference",
        "Inference speed on validation set: <10 seconds",
        "Output accuracy: PyTorch vs ONNX difference <1e-5 (tolerance)",
        "Optional: model quantization for speed (fp16 or int8)",
        "Script outputs: 'ONNX export successful. Inference: Xs, Accuracy: \u2713'"
      ],
      "verification": "python src/export_onnx.py --checkpoint models/checkpoints/best_model.pt --output models/final/model.onnx && python src/test_onnx.py --model models/final/model.onnx",
      "why": "Competition requires ONNX format for fast inference. Must meet <10s time constraint.",
      "files_touched": [
        "src/export_onnx.py",
        "src/test_onnx.py",
        "models/final/model.onnx"
      ],
      "status": "todo",
      "attempts": 0,
      "errors": []
    },
    {
      "id": "ML-009",
      "title": "Competition submission package",
      "description": "Create solution.py implementing PredictionModel class interface, package with model.onnx into submission.zip.",
      "acceptance_criteria": [
        "solution.py at project root implements PredictionModel class",
        "Loads models/final/model.onnx correctly",
        "Implements .predict(datapoint: DataPoint) -> np.ndarray method",
        "Uses utils.py DataPoint class (unchanged from starter pack)",
        "scripts/make_submission.py creates submission.zip",
        "submission.zip contains: solution.py, model.onnx (any other model artifacts)",
        "Local test: python solution.py runs without errors",
        "README updated with submission instructions"
      ],
      "verification": "python scripts/make_submission.py && unzip -l submission.zip && python solution.py",
      "why": "Competition submission format required. Must match interface spec exactly or submission fails.",
      "files_touched": [
        "solution.py",
        "scripts/make_submission.py",
        "submission.zip"
      ],
      "status": "todo",
      "attempts": 0,
      "errors": []
    },
    {
      "id": "ML-010",
      "title": "Documentation and reproducibility",
      "description": "Complete README, training config, EDA notebook, code quality checks (linting, tests).",
      "acceptance_criteria": [
        "README.md updated with: env setup, data download, training command, evaluation results, submission steps",
        "config/train_config.yaml: all hyperparameters documented",
        "notebooks/exploratory_analysis.ipynb: EDA on train/valid data",
        "All Python files have docstrings and type hints",
        "Code passes: black --check src/ && flake8 src/ --max-line-length=120",
        "Tests pass: pytest tests/ -v --cov=src --cov-report=term-missing",
        "Test coverage >80% on data_loader, model, training modules",
        "Git repo clean: no uncommitted changes"
      ],
      "verification": "black --check src/ && flake8 src/ --max-line-length=120 && pytest tests/ -v --cov=src --cov-report=term && git status",
      "why": "Ensure reproducibility. Code quality for collaboration. Documentation for future reference.",
      "files_touched": [
        "README.md",
        "config/train_config.yaml",
        "notebooks/exploratory_analysis.ipynb",
        "src/*",
        "tests/*"
      ],
      "status": "todo",
      "attempts": 0,
      "errors": []
    }
  ]
}